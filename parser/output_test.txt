Metadata of the chapter that will be visualized in SpringerLink Book Title Series Title Chapter Title Copyright Year Hyperparameter Tuning for Machine and Deep Learning with R Case Study IV: Tuned Reinforcement Learning (in Python) 2023 Copyright HolderName The Author(s) Corresponding Author Family Name Zaefferer Particle Given Name Prefix Suffix Role Division Organization Address Division Martin Bartz & Bartz GmbH Gummersbach, Germany Institute for Data Science, Engineering, and Analytics Organization TH Köln Address Division Steinmüllerallee 1, 51643, Gummersbach, Germany Organization Duale Hochschule Baden-Württemberg Ravensburg Address Email Family Name Particle Given Name Prefix Suffix Role Division Organization Address Email Ravensburg, Germany zaefferer@dhbw-ravensburg.de Chandrasekaran Sowmya Institute for Data Science, Engineering, and Analytics TH Köln Steinmüllerallee 1, 51643, Gummersbach, Germany sowmya.chandrasekaran@th-koeln.de Similar to the example in Chap. 10, which considered tuning a Deep Neural Network (DNN), this chapter also deals with neural networks, but focuses on a different type of learning task: reinforcement learning. This increases the complexity, since any evaluation of the learning algorithm also involves the simulation of the respective environment. The learning algorithm is not just tuned with a static data set, but rather with dynamic feedback from the environment, in which an agent operates. The agent is controlled via the DNN. Also, the parameters of the reinforcement learning algorithm have to be considered in addition to the network parameters. Based on a simple example from the Keras documentation, we tune a DNN used for reinforcement learning of the inverse pendulum environment toy example. As a bonus, this chapter shows how the demonstrated tuning tools can be used to interface with and tune a learning algorithm that is implemented in Python. Author Abstract 
f o o r P r o h t u A 1 2 3 4 5 6 7 8 9 10 11 12 Chapter 11 Case Study IV: Tuned Reinforcement Learning (in PYTHON) Martin Zaefferer and Sowmya Chandrasekaran Abstract Similar to the example in Chap. 10, which considered tuning a Deep Neural Network (DNN), this chapter also deals with neural networks, but focuses on a different type of learning task: reinforcement learning. This increases the complexity, since any evaluation of the learning algorithm also involves the simulation of the respective environment. The learning algorithm is not just tuned with a static data set, but rather with dynamic feedback from the environment, in which an agent operates. The agent is controlled via the DNN. Also, the parameters of the reinforcement learning algorithm have to be considered in addition to the network parameters. Based on a simple example from the Keras documentation, we tune a DNN used for reinforcement learning of the inverse pendulum environment toy example. As a bonus, this chapter shows how the demonstrated tuning tools can be used to interface with and tune a learning algorithm that is implemented in Python. F O O R D P E T C E R R O C N U M. Zaefferer · S. Chandrasekaran Institute for Data Science, Engineering, and Analytics, TH Köln, Steinmüllerallee 1, 51643 Gummersbach, Germany e-mail: sowmya.chandrasekaran@th-koeln.de Supplementary Information The online version contains supplementary material available at https://doi.org/10.1007/978-981-19-5170-1_11. M. Zaefferer (B) Bartz & Bartz GmbH, Gummersbach, Germany e-mail: zaefferer@dhbw-ravensburg.de M. Zaefferer Duale Hochschule Baden-Württemberg Ravensburg, Ravensburg, Germany © The Author(s) 2023 E. Bartz et al. (eds.), Hyperparameter Tuning for Machine and Deep Learning with R, https://doi.org/10.1007/978-981-19-5170-1_11 1 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A AQ1 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 2 M. Zaefferer and S. Chandrasekaran 11.1 Introduction In this chapter, we will demonstrate how a reinforcement learning algorithm can be tuned. In reinforcement learning, we consider a dynamic learning process, rather than a process with fixed, static data sets like in typical classification tasks. To simplify things for a second, let us consider an example: A mobile robot (agent) is placed in a room (environment). The state of the agent is the position of the robot. The reward may be based on the distance traveled toward a target position. Different movements of the robot are the respective actions. The learning task considers an agent, which operates in an environment. In each timestep, the agent decides to take a certain action. This action is fed to the environment, and causes a change from a previous state to a new state. The environment also determines a reward for the respective action. After that, the agent will decide the next action to take, based on received reward and the new state. The learning goal is to find an agent that accumulates as much reward as possible. In this case, our neural network can be used to map from the current state to a new action. Thus, it presents a controller for our robot agent. The weights of this neural network have to be learned in some way, taking into account the received rewards. Compared to Chap. 10, this leads to a somewhat different scenario: Data is usually gathered in a dynamic process, rather than being available from the start.1 In fact, initially, we may not have any data. We acquire data during the learning process, by observing states/actions/rewards in the environment. F O O R D P E T C E R R O C N U We largely rely on the same software as in the previous chapters. That is, we use the same tuning tools. As in Chap. 10, we use Keras and TensorFlow to implement the neural networks. However, we will perform the complete learning task within Python, using the R package reticulate to explicitly interface between the R-based tuner and the Python-based learning task (rather than implicitly via R’s keras package). On the one hand, this will demonstrate how to interface with different programming languages (i.e., if your model is not trained in R). On the other hand, this is a necessary step, because the respective environment is only available in Python (i.e., the toy problem). 1 Although it has to be noted that somewhat similar dynamics may occur, e.g., when learning a classification model with streaming data. For the sake of readability, the complete code will not be printed within the main text, but is available as supplementary material. 11.2 Materials and Methods 11.2.1 Software 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 11 Case Study IV: Tuned Reinforcement Learning (in Python) 3 Inital state Goal / final state Pendulum in goal posi(cid:2)on joint joint applied torque applied torque (small) Fig. 11.1 Initial and goal state of the 2D inverted pendulum problem 11.2.2 Task Environment: Inverted Pendulum The example we investigate is based on a Keras tutorial by Singh (2020). This relies on a toy problem that is often used to test or demonstrate reinforcement learning algorithms: the frictionless inverted pendulum. More specifically, we use the implementation of the inverse problem which is provided by OpenAI Gym2 denoted as Pendulum-v0. Pendulum in poten(cid:2)al start posi(cid:2)on F O O R D P E T C E R R O C N U The state of this problem’s environment is composed of three values: the sine and cosine of the pendulum angle, and the angular velocity. The action is the applied torque (with a sign representing a change of direction), and the reward is computed with −(angle2 + 0.1 ∗ velocity2 + 0.001 ∗ torque2). This ensures that the learning process sees the largest rewards if the pendulum is upright (angles are close to zero), moving slowly (small angular velocities), with little effort (small torques). The inverted pendulum is a simple 2D-physical simulation. Initially, the pendulum hangs downwards, and has to be swung upwards, by applying force at the joint, either to the left or right. Once the pendulum is oriented upwards, it has to be balanced there as long as possible. This situation is shown in Fig. 11.1. 2 OpenAI Gym is a collection of reinforcement learning problems; see https://github.com/openai/ gym for details such as installation instructions. 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 4 M. Zaefferer and S. Chandrasekaran 11.2.3 Learning Algorithm The learning algorithm also makes use of experience replay, which represents a collection (or buffer) of tuples consisting of states, actions, rewards, and new states. This allows learning from a set of previously experienced agent-environment interactions, rather than just updating the model with the most recent ones. The respective target networks are copies of these two networks. They use the same architecture and weights, which are not directly trained for the target networks but are instead updated via cloning them from the original networks regularly during the learning process. These concepts (the actor-critic concept and target networks) are intended to stabilize network training. Largely, we leave the algorithm used in the original Keras tutorial as is (Singh 2020). In fact, this algorithm follows the concept of the Deep Deterministic Policy Gradient (DDPG) algorithm by Lillicrap et al. (2015). We will not go into all details of the algorithm, but will note some important aspects for setting up the tuning procedure. The learning algorithm essentially uses four different networks: an actor network, a target actor network, a critic network, and a target critic network. The actor network represents the policy of the agent: mapping from states to actions. The critic network tries to guess the value (in terms of future rewards) of the current state/action pair, thus providing a baseline to compare the actor against. That is, the critic network maps from states and actions to a kind of estimated reward value. F O O R D P E T C E R R O C N U • Respectively, these parameters have all been changed from the original, hardcoded values in the Keras tutorial. The original (default) values in the tutorial are num_hidden=256, critic_lr=0.002, actor_lr=0.001, gamma=0.99, tau=0.005, activation =“relu”. The learning algorithm and task environment are processed with Python code, in the file run.py. This is to a large extent identical to the Keras tutorial (Singh 2020). Here, we explain the relevant changes, showing some snippets from the code. Importantly, the arguments consist of the parameters that will be tuned, as well as max_episodes (number of learning episodes that will be run) and a seed for the random number generator. • The complete code is wrapped into a function, which will later be called from R 11.3 Setting up the Tuning Experiment def run_ddpg(num_hidden,critic_lr,actor_lr, gamma,tau,activation,max_episodes,seed): 11.3.1 File: run.py via the reticulate interface. 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 11 Case Study IV: Tuned Reinforcement Learning (in Python) 5 • Note that we vary only the size of the largest layers in the networks (default: 256). Especially, the critic has smaller layers that collect the respective inputs (states, actions). These remain unchanged. during automated tuning. 11.3.2 Tuned Parameters dures we employ assume smaller values to be better. More details on the tuned parameters are given next. • Via the argument activation, we only replace the activation functions of the internal layers, not the activation function of the final output layers. env.seed(seed) tf.random.set_seed(seed) np.random.seed(seed) • Note that all reward values we consider will be negated, since most of the proce• We remove the plots from the tutorial code, as these are not particularly useful • To make sure that results are reproducible, we set the random number generator seeds (for the reinforcement learning environment, TensorFlow, and NumPy): • Finally, we return the variable avg_reward_list, which is the average reward of the last 40 episodes. This returned value will be the objective function value that our tuner Sequential Parameter Optimization Toolbox (SPOT) observes. F O O R D P E T C E R R O C N U In the previous Sect. 11.3.1, we already brieﬂy introduced the tuned parameters and their default values: num_hidden=256, critic_lr=0.002, actor_lr=0.001, gamma=0.99, tau=0.005, activation=“relu”. Some of these we may recognize, matching parameters of neural networks that we considered throughout other parts of this book: num_hidden corresponds to the previously discussed units, but is a scalar value (it is reused to define the size of all the larger layers in all networks). Instead of a single learning_rate, we have separate learning rates for the actor and critic networks, critic_lr, and actor_lr. The parameter gamma is new, as it is specific to actor-critic learning algorithms: it represents a discount factor which is applied to estimated rewards as a multiplicator. The parameter tau is also new, representing a multiplicator that is used when updating the weights of the target networks. Finally, activation is the activation function (here: shared between all internal layers). The parameters and their bounds are summarized in Table 11.1. The following code snippet shows the code used to define this parameter search space for SPOT in R. List of configurations 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 136 137 138 139 140 141 142 6 M. Zaefferer and S. Chandrasekaran Table 11.1 The hyperparameters for our reinforcement learning example. Note that the defaults and bounds concern the actual scale of each parameter (not transformed). Defaults denote the values from the original Keras tutorial, not the formal defaults from the Keras function interfaces Name Lower bound Upper bound Default Type num_hidden critic_lr actor_lr gamma tau activation ## configuration for the tuning problem cfg <list( Scale transformation 1), -1, -5, -5, -1, -4, -4, c(8, -0.3, c(256, "gamma","tau","activation"), 8 1e − 5 1e − 5 0.5 1e − 4 256 1e − 1 1e − 1 1 1e − 0 Integer Double Double Double Double Factor x 10x 10x 1 − 10x 10x relu ## Names of the parameters tunepars = c("num_hidden","actor_lr","critic_lr", 256 0.002 0.001 0.99 0.001 relu, swish, sigmoid ## their lower bounds lower = ## their upper bounds upper = ## their type type = ## transformations to apply transformations = F O O R D P E T C E R R O C N U ## another parameter that will not be tuned, but is fixed fixpars = list(max_episodes=50L), ## specify levels of categorical parameters ## (i.e., to translate from integers to these factor levels): factorlevels = list(activation=c("relu","swish","sigmoid")), ## not used in this example ## (specify parameters that are relative to other parameters) relpars = list() trans_1minus10pow,trans_10pow,trans_id), c(trans_id,trans_10pow,trans_10pow, 11.3.3 Further Configuration of SPOT of the learning process to 50 episodes. ) SPOT is configured to use 300 evaluations, which are spent as follows: Each evaluation is replicated (evaluated repeatedly) five times, to account for noise. Noise is a substantial issue in reinforcement learning cases like this one. Note that we set a single fixed parameter, max_episodes, limiting the evaluation c("integer","numeric","numeric","numeric","numeric","factor"), 0, 3), 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 11 Case Study IV: Tuned Reinforcement Learning (in Python) 7 result <spot(fun = objf, Arguments for calling SPOT The respective configuration and function call is 30 different configurations are tested in the initial design, leading to 150 evaluations (including the replications). The remaining 150 evaluations are spent by the iterative search procedure of SPOT. Due to replications, this implies that 30 further configurations are tested. Also, due to the stochastic nature of the problem, we set the parameter noise=TRUE. For the sake of reproducibility, random number generator seeds are specified (seedSPOT, seedFun). Each replication will work with a different random number generator seed (iterated, starting from seedFun). The employed surrogate model is Kriging (a.k.a. Gaussian process regression), which is configured to use the so-called nugget effect (useLambda=TRUE), but no re-interpolation (reinterpolate=FALSE). In each iteration after the initial design, a Differential Evolution algorithm is used to search the surrogate model for a new, promising candidate. The Differential Evolution algorithm is allowed to spend 2400 evaluations of the surrogate model in each iteration of SPOT. lower=cfg$lower, upper=cfg$upper, control = list(types=cfg$type, F O O R D P E T C E R R O C N U optimizerControl=list(funEvals= funEvals=300, plots=TRUE, optimizer=optimDE, noise=TRUE, seedSPOT=1, seedFun=1, designControl=list(size=5*length(cfg$lower), To determine how well the tuning worked, we perform a short validation experiment at the end. There, we spend 10 replications to evaluate the best found solution. We also spend more episodes for this test (i.e., max_episodes=100). This provides a less replicates=5, model=buildKriging, modelControl=list(target="ei",useLambda=TRUE, 11.3.4 Post-processing and Validating the Results 400*length(cfg$lower)) reinterpolate=FALSE), replicates=5), ) ) 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 8 M. Zaefferer and S. Chandrasekaran noisy and more reliable estimate of our solution’s performance, compared to the respective performance of the default settings from the tutorial (see Table 11.1). 11.4 Results load("supplementary/ch11-caseStudyIV/resultFile.RData") boxplot(best_real_y,default_y, Note that this step requires a bit of data processing, where we first aggregate our result data set by computing mean objective values (i.e., over the 5 replications), to determine which configuration was evaluated to work best on average. Table 11.2 compares the parameters of the best solution found during tuning with those of the defaults from the tutorial. It also lists the respective performance (average reward) and its standard deviation. We can load the result file created after tuning to create a visual impression of this comparison (Fig. 11.2). Interestingly, much smaller size of the dense layers (num_hidden=64) seems to suffice for the tuned solution. The larger tutorial network uses 256 units. The tuned algorithm also uses a larger learning rate for the critic network, compared to the actor network. The parameters gamma and tau deviate strongly from the respective defaults. names=c("tuned","default"), xlab="performance (-reward)", horizontal=TRUE) F O O R D P E T C E R R O C N U 256 0.00200 0.00100 0.99000 0.00100 relu 183.62 37.49 64 0.00349 0.00074 0.93668 0.01481 swish 169.86 27.60 num_hidden critic_lr actor_lr gamma tau activation Average negated reward st. dev. of avg. neg. reward Table 11.2 The hyperparameter values of the best solution found during tuning, compared against those of the defaults from the Keras tutorial by Singh (2020). It also lists the respective performance (mean neg. reward) and its standard deviation. Mean and standard deviation are computed over 10 replications, evaluated with 100 episodes Default Variable name Tuned 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard 
f o o r P r o h t u A 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 11 Case Study IV: Tuned Reinforcement Learning (in Python) 9 t l u a f e d 200 140 160 220 180 240 d e n u t performance (−reward) 11.5 Severity: Validating the Results Fig. 11.2 Boxplot comparing the default and tuned configurations of the reinforcement learning process, in terms of their negated reward Let us proceed to analyze the average negated reward attained between the tuned and default parameters using severity. The pre-experimental runs indicate that the difference is ¯x = 13.76. Because this value is positive, we can assume that the tuned solution is superior. The standard deviation is sd = 32.67. Based on Eq. 5.14, and with α = 0.05, β = 0.2, and (cid:4) = 40, we can determine the number of runs for the full experiment. F O O R D P E T C E R R O C N U The decision based on the p-value of 0.0915 is to not reject H0. Considering a target relevant difference (cid:4) = 40, the severity of not rejecting H0 is 0.99, and thus it strongly supports the decision of not rejecting the H0. The corresponding severity plot is shown in Fig. 11.3. Analyzing the results of hypothesis testing and severity as shown in Table 11.3, the differences in terms of parameter values do not seem to manifest in the performance values. It can be observed in Table 11.2 that a comparatively minor difference in mean performance is observed, while the difference in standard deviation is a bit more pronounced. However, this cannot be deemed as statistically significant relevance. For a relevant difference of 40, approximately 8 completing runs per algorithm are required. Hence, we can proceed directly to evaluate the severity as sufficient runs have already been performed. Overall, this matches well with what we see from a more detailed look at the SPOTMisc::plot_parallel(resultpp) tuning results (Fig. 11.4): 521645_1_En_11_Chapter (cid:2) TYPESET DISK LE (cid:2) CP Disp.:5/9/2022 Pages: xxx Layout: T1-Standard AQ2 
