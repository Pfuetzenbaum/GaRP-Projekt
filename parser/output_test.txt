
f o o r P r o h t u A 6 M. Zaeﬀerer and D. Horn deﬁnitions and assumptions of our kernels. Hence, we suggest a simple twodimensional quadratic function f (x) = (x1 − d)2 + (cid:8) 0 (x2 − 0.5)2 + b else if x1 ≤ c . The function’s behavior (see Fig. 1) is deﬁned by the constants b, c and d. The constant b controls whether the Imp-kernel is a good match, c controls the size of the active region and d controls the location of the optimum. The function is inﬂuenced by the hierarchical variable x2 only if x1 > c and does have a discontinuity at x1 = c. For b = 0, the function is continuous at x2 = 0.5. Hence, the if-else term of f (x) yields identical results if x2 = 0.5 and if δ2(x) = f alse. In this case, the assumption of the Imp-kernel is fulﬁlled, i.e., the kernel deﬁnition matches the problem structure. The Imp-kernel should learn to impute ρ = 0.5. We identiﬁed ﬁve situations with diﬀerent expected performances. (A) d < c (the optimum is in the inactive region of x2 at x1 = d, x2 ∈ R) and b = 0 (imputation potentially proﬁtable). The function is unimodal. (B) d < c (the optimum is in the inactive region at x1 = d, x2 ∈ R) but b > 0 (imputation potentially unproﬁtable). The function is unimodal. (C) d > c (the optimum is in the active region at x1 = d, x2 = 0.5) and b = 0 (imputation potentially proﬁtable). The function is bimodal. (D) d > c (the optimum is in the active region at x1 = d, x2 = 0.5) and b = 0.1 (imputation potentially unproﬁtable) and b < (c − d)2. The function is bimodal. The discontinuity at c is not as important, since the optimum is remote from it. (E) d > c (the optimum is in the active region at x1 = c, x2 ∈ R) and b = 0.1 (imputation potentially unproﬁtable) and b > (c − d)2. The function is bimodal. The discontinuity at c has to be approximated well, since the optimum is at x1 = c. b = 0.1, c = 0.4, d = 0.7 b = 0.0, c = 0.6, d = 0.1 value 1.00 0.75 0.50 0.25 2 x 1.0 0.8 0.6 0.4 0.2 0.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 x1 0.2 0.4 0.6 0.8 1.0 Fig. 1. Visualization of the test function, the optimum is marked in yellow. (Color ﬁgure online) 
f o o r P r o h t u A A First Analysis of Kernels for Kriging-Based Optimization 7 Covering all of these ﬁve situations, we tested all combinations of the values b = {0, 0.1}, c = {0.2, 0.4, 0.6, 0.8}, and d = {0.1, 0.3, 0.5, 0.7, 0.9}. To estimate model quality, we measured the model’s Root Mean Squared Error (RMSE). The models were trained with 10, the error was estimated on 1 000 uniform random samples. The Kriging model was trained with the CEGO package in R [15,16]. It was conﬁgured to use the nugget eﬀect and reinterpolation. The Dividing Rectangles algorithm [17] was chosen to optimize the model parameters via 200 likelihood evaluations. We used all kernels from Sect. 4 and a standard exponential kernel with square deviation in each dimension (which does not incorporate hierarchical information), denoted as the Stankernel. The same type of model was used in the SMBO algorithm from the CEGO package. The search was limited to 10 evaluations of f (x), due its low diﬃculty, low dimensionality and assumed cost. The search was initialized with three uniform random samples. Based on the model, the EI criterion was optimized by DE [11]. We used the DEoptim package [18] with 10 000 EI evaluations per iteration and used default parameters otherwise. Each experiment was repeated 100 times, with 100 unique random seeds (one per replication). We recorded the difference between the best found and the optimal function value (suboptimality) for each replication. 6 Results First, we analyze the model quality produced by the diﬀerent kernels. Figure 2 shows the median RMSE value for all parameter constellations and kernels. Clearly, the ﬁt of the Stan-kernel is inferior to most specialized hierarchical kernels for almost all parameter constellations, especially if b = 0.1. If b = 0, the assumption of the Imp-kernel is fulﬁlled. Hence, both the Impand the ImpArc-kernel produce a better ﬁt than most other kernels. However, for b = 0.1, the Imp-kernel mostly has the second or third worst performance. Only the Stan-kernel and sometimes the IcoCorrected-kernel perform worse. The Arcand the Ico-kernel achieve very similar performances in most cases, with nearto-best performance if b = 0.1. The ImpArc-kernel, combining the advantages of the Arcand the Imp-kernel, has a good, sometimes best ﬁt in all situation, for both b ∈ {0, 0.1}. Contrarily, the IcoCorrected-kernel has a rather poor ﬁt in several cases, sometimes even worse than the Stan-kernel. Overall, diﬀerences between kernels tend to disappear for large values of c, which is to be expected due to the reduced inﬂuence of the hierarchical variable x2. To get a better understanding of the kernels, we visualize an example for Situation E with (b, c, d) = (0.1, 0.4, 0.7). Figure 3 shows line plots for the test function as well as ﬁtted models for all six kernels, trained with ten uniform random samples. Here, the global optimum is at x1 = c = 0.4, i.e., at the jump discontinuity. The function value of the global optimum (0.09) is only slightly better than the value of the local optimum (0.1) at (x1 = 0.7, x2 = 0.5). Hence, to ﬁnd the global optimum, it is important to model the discontinuity well. 